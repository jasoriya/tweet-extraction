{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Import libraries\nimport os\nimport re\nimport string\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tokenizers\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/tf-roberta/pretrained-roberta-base.h5\n/kaggle/input/tf-roberta/merges-roberta-base.txt\n/kaggle/input/tf-roberta/config-roberta-base.json\n/kaggle/input/tf-roberta/vocab-roberta-base.json\n/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n/kaggle/input/tweet-sentiment-extraction/test.csv\n/kaggle/input/tweet-sentiment-extraction/train.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-processing and Transformation"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training set has {} data points\".format(len(train)))\nprint(\"Testing set has {} data points\".format(len(test)))","execution_count":4,"outputs":[{"output_type":"stream","text":"Training set has 27481 data points\nTesting set has 3534 data points\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n\n                         selected_text sentiment  \n0  I`d have responded, if I were going   neutral  \n1                             Sooo SAD  negative  \n2                          bullying me  negative  \n3                       leave me alone  negative  \n4                        Sons of ****,  negative  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549e992a42</td>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>Sooo SAD</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>088c60f138</td>\n      <td>my boss is bullying me...</td>\n      <td>bullying me</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9642c003ef</td>\n      <td>what interview! leave me alone</td>\n      <td>leave me alone</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>Sons of ****,</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"       textID                                               text sentiment\n0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n3  01082688c6                                        happy bday!  positive\n4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f87dea47db</td>\n      <td>Last session of the day  http://twitpic.com/67ezh</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>96d74cb729</td>\n      <td>Shanghai is also really exciting (precisely -...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eee518ae67</td>\n      <td>Recession hit Veronique Branquinho, she has to...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01082688c6</td>\n      <td>happy bday!</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33987a8ee5</td>\n      <td>http://twitpic.com/4w75p - I like it!!</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Check for NaN values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"textID           0\ntext             1\nselected_text    1\nsentiment        0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isna().sum()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"textID       0\ntext         0\nsentiment    0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since there is only one NaN value, let's drop it\n# train = train.dropna(axis=0).reset_index(drop=True)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training set has {} data points\".format(len(train)))\nprint(\"Testing set has {} data points\".format(len(test)))\ntrain.isna().sum()","execution_count":10,"outputs":[{"output_type":"stream","text":"Training set has 27481 data points\nTesting set has 3534 data points\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"textID           0\ntext             1\nselected_text    1\nsentiment        0\ndtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Removing punctuations & stopwords, or not? "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if punctuation appears in selected_text\nselected_text_has_punctuation = train.selected_text.str.extract(\n                                                        r'([{}]+)'.format(\n                                                            re.escape(\n                                                                string.punctuation)))\n# number of selected_text with punctuations\nselected_text_has_punctuation.isna().sum() ","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"0    10982\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# observing some tweets whose selected_text contain punctuations \ntrain.loc[selected_text_has_punctuation.dropna().index].head()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"        textID                                               text  \\\n0   cb774db0d1                I`d have responded, if I were going   \n4   358bd9e861   Sons of ****, why couldn`t they put them on t...   \n5   28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n9   fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n10  2339a9b08b   as much as i love to be hopeful, i reckon the...   \n\n                                        selected_text sentiment  \n0                 I`d have responded, if I were going   neutral  \n4                                       Sons of ****,  negative  \n5   http://www.dothebouncy.com/smf - some shameles...   neutral  \n9                        Wow... u just became cooler.  positive  \n10  as much as i love to be hopeful, i reckon the ...   neutral  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>Sons of ****,</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>28b57f3990</td>\n      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>fc2cbefa9d</td>\n      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n      <td>Wow... u just became cooler.</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2339a9b08b</td>\n      <td>as much as i love to be hopeful, i reckon the...</td>\n      <td>as much as i love to be hopeful, i reckon the ...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"- Punctuation seems to appear in quite a lot of our extracted examples. I'll not remove punctuations for this dataset.\n- Also, I need to preserve stopwords as it can be seen in the above `neutral` sentiment that the tweet *text* has been extracted as-is in the *selected_text*. "},{"metadata":{},"cell_type":"markdown","source":"### Deciding max *text* length"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text.str.len().max()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"141.0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 148","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenizer"},{"metadata":{},"cell_type":"markdown","source":"The pretrained RoBERTa model and tokenizer are from huggingface [transformers](https://huggingface.co/transformers/main_classes/model.html?highlight=save_pretrained) library. They can be downloaded by using the `from_pretrained()` method or attached to a kaggle kerned from [here](https://www.kaggle.com/cdeotte/tf-roberta)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, data_df, tokenizer, train=True, max_len=96):\n        self.data = data_df.dropna(axis=0).reset_index(drop=True)\n        self.is_train = True if train else False\n        self.sentiment_tokens = {\n            'positive': tokenizer.encode('positive').ids[0], \n            'negative': tokenizer.encode('negative').ids[0],\n            'neutral': tokenizer.encode('neutral').ids[0]\n        }\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def ByteLevelBPEPreprocessor(self, text, selected_text, sentiment):\n        \n        text = \" \" + \" \".join(text.split())\n        enc = self.tokenizer.encode(text)\n        s_tok = self.sentiment_tokens[sentiment]\n        \n        # Get InputIDs\n        input_ids = np.ones((self.max_len),\n                            dtype = 'int32')\n        input_ids[:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n\n        # Get Attention mask\n        attention_mask = np.zeros((self.max_len),\n                                  dtype='int32')\n        attention_mask[:len(enc.ids)+5] = 1\n        \n        if self.is_train:\n            selected_text = \" \".join(selected_text.split())\n            idx = text.find(selected_text)\n            char_tokens = np.zeros((len(text)))\n            char_tokens[idx:idx+len(selected_text)] = 1\n            # if text has ' ' prefix\n            if text[idx-1] == ' ': \n                char_tokens[idx-1] = 1\n                \n            # Get start and end token for selected_text in input IDs\n            start_tokens = np.zeros((self.max_len),\n                                    dtype='int32')\n            end_tokens = np.zeros((self.max_len),\n                                  dtype='int32')\n            ptr_idx = 0\n            label_idx = list()\n            for i, enc_id in enumerate(enc.ids):\n                sub_word = self.tokenizer.decode([enc_id])\n                if sum(char_tokens[ptr_idx:ptr_idx+len(sub_word)]) > 0:\n                    label_idx.append(i)\n                ptr_idx += len(sub_word)\n            if label_idx:\n                # + 1 as we added prefix before\n                start_tokens[label_idx[0] + 1] = 1\n                end_tokens[label_idx[-1] + 1] = 1\n            return input_ids, attention_mask, start_tokens, end_tokens\n        \n        return input_ids, attention_mask\n            \n    def __call__(self):\n        data_len = len(self.data)\n        input_ids = np.ones((data_len, self.max_len), \n                            dtype='int32')\n        attention_mask = np.zeros((data_len, self.max_len), \n                                  dtype='int32')\n        token_type_ids = np.zeros((data_len, self.max_len),\n                                  dtype='int32')\n        if self.is_train:\n            start_tokens = np.zeros((data_len, self.max_len),\n                                    dtype='int32')\n            end_tokens = np.zeros((data_len, self.max_len),\n                                  dtype='int32')\n        for i, row in tqdm(self.data.iterrows(), total=len(self.data)):\n            out = self.ByteLevelBPEPreprocessor(\n                row['text'], \n                row['selected_text'] if self.is_train else None, \n                row['sentiment']\n            )\n            if self.is_train:\n                input_ids[i], attention_mask[i], start_tokens[i], end_tokens[i] = out\n            else:\n                input_ids[i], attention_mask[i] = out\n        if self.is_train:\n            return input_ids, attention_mask, token_type_ids, start_tokens, end_tokens\n        return input_ids, attention_mask, token_type_ids","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformerQA:\n    def __init__(self, max_len, model_path, tokenizer, fit=True):\n        self.max_len = max_len\n        self.model_path = model_path\n        self.tokenizer = tokenizer\n        \n    def roberta_model(self):\n        input_ids = tf.keras.layers.Input((self.max_len,),\n                                          dtype=tf.int32)\n        attention_mask = tf.keras.layers.Input((self.max_len,),\n                                               dtype=tf.int32)\n        token_type_ids = tf.keras.layers.Input((self.max_len,),\n                                               dtype=tf.int32)\n\n        config = RobertaConfig.from_pretrained(\n            os.path.join(self.model_path, 'config-roberta-base.json')\n        )\n        roberta_model = TFRobertaModel.from_pretrained(\n            os.path.join(self.model_path, 'pretrained-roberta-base.h5'),\n            config=config\n        )\n        x = roberta_model(inputs=input_ids,\n                          attention_mask=attention_mask,\n                          token_type_ids=token_type_ids)\n\n        x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n        x1 = tf.keras.layers.Conv1D(1,1)(x1)\n        x1 = tf.keras.layers.Flatten()(x1)\n        x1 = tf.keras.layers.Activation('softmax')(x1)\n\n        x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n        x2 = tf.keras.layers.Conv1D(1,1)(x2)\n        x2 = tf.keras.layers.Flatten()(x2)\n        x2 = tf.keras.layers.Activation('softmax')(x2)\n\n        model = tf.keras.models.Model(\n            inputs=[input_ids, attention_mask, token_type_ids], \n            outputs=[x1,x2]\n        )\n        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n        model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n        return model\n    \n    def jaccard(self, str1, str2):\n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        if (len(a)==0) & (len(b)==0): return 0.5\n        c = a.intersection(b)\n        return float(len(c)) / (len(a) + len(b) - len(c))\n    \n    def get_model_selected_text(self, data_df, preds_start, preds_end):\n        st_list = []\n        for k in range(len(data_df)):\n            idx_start = np.argmax(preds_start[k,])\n            idx_end = np.argmax(preds_end[k,])\n            if idx_start > idx_end:\n                st = data_df.loc[k,'text']\n#                 if data_df.loc[k, 'sentiment'] != 'neutral':\n#                     st = st.split()[idx_start]\n            else:\n                text = \" \" + \" \".join(data_df.loc[k,'text'].split())\n                enc = self.tokenizer.encode(text)\n                st = self.tokenizer.decode(enc.ids[idx_start-1:idx_end])\n            st_list.append(st)\n        return st_list\n    \n    def fit(self, train_df, input_ids, attention_mask, \n            token_type_ids, start_tokens, end_tokens, \n            stratify_y, VER='v0', verbose=1):\n        avg_score = []\n        oof_start = np.zeros((input_ids.shape[0],\n                              self.max_len))\n        oof_end = np.zeros((input_ids.shape[0],\n                            self.max_len))\n        skf = StratifiedKFold(n_splits=5,\n                              shuffle=True,\n                              random_state=42)\n        for fold, (idxT,idxV) in enumerate(skf.split(input_ids,\n                                                     stratify_y)):\n            print('Training FOLD {}:'.format(fold+1))\n            K.clear_session()\n            model = self.roberta_model()\n            sv = tf.keras.callbacks.ModelCheckpoint(\n                '{}-roberta-{}.h5'.format(VER, fold), \n                monitor='val_loss', \n                verbose=verbose, \n                save_best_only=True,\n                save_weights_only=True, \n                mode='auto', \n                save_freq='epoch'\n            )\n            model.fit([input_ids[idxT,], \n                       attention_mask[idxT,], \n                       token_type_ids[idxT,]], \n                      [start_tokens[idxT,], end_tokens[idxT,]],\n                      epochs=3, \n                      batch_size=32, \n                      verbose=verbose, \n                      callbacks=[sv],\n                      validation_data=(\n                          [\n                              input_ids[idxV,],\n                              attention_mask[idxV,],\n                              token_type_ids[idxV,]\n                          ], \n                          [start_tokens[idxV,], end_tokens[idxV,]]\n                      )\n                     )\n            # Load best saved model from disk\n            print('Loading model...')\n            model.load_weights('{}-roberta-{}.h5'.format(VER, fold))\n            \n            # Predicting OOF samples\n            print('Predicting OOF...')\n            oof_start[idxV,],oof_end[idxV,] = model.predict(\n                [\n                    input_ids[idxV,],\n                    attention_mask[idxV,],\n                    token_type_ids[idxV,]\n                ],\n                verbose=verbose\n            )\n            \n            pred_df = train_df.loc[idxV].reset_index(drop=True)\n            pred_df['oof_st'] = self.get_model_selected_text(\n                data_df=pred_df,\n                preds_start=oof_start[idxV,],\n                preds_end=oof_end[idxV,]\n            )\n            fold_val_score = pred_df.apply(\n                lambda x: self.jaccard(x['selected_text'], \n                                       x['oof_st']\n                                      ),\n                axis=1\n            ).mean()\n            avg_score.append(fold_val_score)\n            print('>>>> FOLD {} Jaccard score = {}'.format(fold+1, \n                                                           fold_val_score))\n    def predict(self, pred_df, input_ids, attention_mask, \n                token_type_ids, n_models, VER='v0', verbose=1):\n        preds_start = np.zeros((input_ids.shape[0],\n                                self.max_len))\n        preds_end = np.zeros((input_ids.shape[0],\n                              self.max_len))\n        for i in range(n_models):\n            K.clear_session()\n            model = self.roberta_model()\n            \n            print('Loading model...')\n            model.load_weights('{}-roberta-{}.h5'.format(VER, i))\n            \n            preds = model.predict(\n                [input_ids, attention_mask, token_type_ids],\n                verbose=verbose\n            )\n            preds_start += preds[0]/n_models\n            preds_end += preds[1]/n_models\n        \n        test_st = self.get_model_selected_text(\n            data_df=pred_df,\n            preds_start=preds_start,\n            preds_end=preds_end\n        )\n        return test_st","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = TweetDataset(train, tokenizer, train=True, max_len=MAX_LEN)\ninput_ids, attention_mask, token_type_ids, start_tokens, end_tokens = train_data()","execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=27480.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f7999f119b49929f91b0fba3e8d073"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = TweetDataset(test, tokenizer, train=False, max_len=MAX_LEN)\ntest_input_ids, test_attention_mask, test_token_type_ids = test_data()","execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=3534.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21ec5a5c7f694db0b671302492b00f31"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"QA_model = TransformerQA(max_len=MAX_LEN, \n                         model_path=PATH, \n                         tokenizer=tokenizer)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"QA_model.fit(train_df=train_data.data, \n             input_ids=input_ids, \n             attention_mask=attention_mask, \n             token_type_ids=token_type_ids, \n             start_tokens=start_tokens, \n             end_tokens=end_tokens, \n             stratify_y=train_data.data.sentiment.values)","execution_count":21,"outputs":[{"output_type":"stream","text":"Training FOLD 1:\nTrain on 21984 samples, validate on 5496 samples\nEpoch 1/3\n21952/21984 [============================>.] - ETA: 0s - loss: 2.2045 - activation_loss: 1.0921 - activation_1_loss: 1.1124\nEpoch 00001: val_loss improved from inf to 1.75551, saving model to v0-roberta-0.h5\n21984/21984 [==============================] - 411s 19ms/sample - loss: 2.2034 - activation_loss: 1.0917 - activation_1_loss: 1.1118 - val_loss: 1.7555 - val_activation_loss: 0.9032 - val_activation_1_loss: 0.8518\nEpoch 2/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.6337 - activation_loss: 0.8407 - activation_1_loss: 0.7930\nEpoch 00002: val_loss improved from 1.75551 to 1.65791, saving model to v0-roberta-0.h5\n21984/21984 [==============================] - 393s 18ms/sample - loss: 1.6339 - activation_loss: 0.8408 - activation_1_loss: 0.7931 - val_loss: 1.6579 - val_activation_loss: 0.8442 - val_activation_1_loss: 0.8132\nEpoch 3/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.4827 - activation_loss: 0.7651 - activation_1_loss: 0.7176\nEpoch 00003: val_loss did not improve from 1.65791\n21984/21984 [==============================] - 393s 18ms/sample - loss: 1.4830 - activation_loss: 0.7651 - activation_1_loss: 0.7180 - val_loss: 1.7415 - val_activation_loss: 0.8789 - val_activation_1_loss: 0.8620\nLoading model...\nPredicting OOF...\n5496/5496 [==============================] - 36s 7ms/sample\n>>>> FOLD 1 Jaccard score = 0.7031862155715445\nTraining FOLD 2:\nTrain on 21984 samples, validate on 5496 samples\nEpoch 1/3\n21952/21984 [============================>.] - ETA: 0s - loss: 2.0852 - activation_loss: 1.0573 - activation_1_loss: 1.0279\nEpoch 00001: val_loss improved from inf to 1.62839, saving model to v0-roberta-1.h5\n21984/21984 [==============================] - 410s 19ms/sample - loss: 2.0841 - activation_loss: 1.0570 - activation_1_loss: 1.0271 - val_loss: 1.6284 - val_activation_loss: 0.8578 - val_activation_1_loss: 0.7697\nEpoch 2/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.6747 - activation_loss: 0.8455 - activation_1_loss: 0.8292\nEpoch 00002: val_loss did not improve from 1.62839\n21984/21984 [==============================] - 393s 18ms/sample - loss: 1.6744 - activation_loss: 0.8457 - activation_1_loss: 0.8287 - val_loss: 1.6350 - val_activation_loss: 0.8460 - val_activation_1_loss: 0.7882\nEpoch 3/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.5038 - activation_loss: 0.7771 - activation_1_loss: 0.7267\nEpoch 00003: val_loss improved from 1.62839 to 1.59848, saving model to v0-roberta-1.h5\n21984/21984 [==============================] - 394s 18ms/sample - loss: 1.5044 - activation_loss: 0.7770 - activation_1_loss: 0.7274 - val_loss: 1.5985 - val_activation_loss: 0.8259 - val_activation_1_loss: 0.7717\nLoading model...\nPredicting OOF...\n5496/5496 [==============================] - 36s 7ms/sample\n>>>> FOLD 2 Jaccard score = 0.707532562130843\nTraining FOLD 3:\nTrain on 21984 samples, validate on 5496 samples\nEpoch 1/3\n21952/21984 [============================>.] - ETA: 0s - loss: 2.1752 - activation_loss: 1.0728 - activation_1_loss: 1.1024\nEpoch 00001: val_loss improved from inf to 1.67778, saving model to v0-roberta-2.h5\n21984/21984 [==============================] - 408s 19ms/sample - loss: 2.1758 - activation_loss: 1.0736 - activation_1_loss: 1.1022 - val_loss: 1.6778 - val_activation_loss: 0.8559 - val_activation_1_loss: 0.8217\nEpoch 2/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.6938 - activation_loss: 0.8661 - activation_1_loss: 0.8278\nEpoch 00002: val_loss improved from 1.67778 to 1.66205, saving model to v0-roberta-2.h5\n21984/21984 [==============================] - 394s 18ms/sample - loss: 1.6932 - activation_loss: 0.8656 - activation_1_loss: 0.8276 - val_loss: 1.6620 - val_activation_loss: 0.8450 - val_activation_1_loss: 0.8171\nEpoch 3/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.5881 - activation_loss: 0.8095 - activation_1_loss: 0.7786\nEpoch 00003: val_loss improved from 1.66205 to 1.61364, saving model to v0-roberta-2.h5\n21984/21984 [==============================] - 394s 18ms/sample - loss: 1.5880 - activation_loss: 0.8091 - activation_1_loss: 0.7789 - val_loss: 1.6136 - val_activation_loss: 0.8342 - val_activation_1_loss: 0.7792\nLoading model...\nPredicting OOF...\n5496/5496 [==============================] - 36s 7ms/sample\n>>>> FOLD 3 Jaccard score = 0.7096439841535017\nTraining FOLD 4:\nTrain on 21984 samples, validate on 5496 samples\nEpoch 1/3\n21952/21984 [============================>.] - ETA: 0s - loss: 2.1854 - activation_loss: 1.0734 - activation_1_loss: 1.1120\nEpoch 00001: val_loss improved from inf to 1.65674, saving model to v0-roberta-3.h5\n21984/21984 [==============================] - 407s 19ms/sample - loss: 2.1856 - activation_loss: 1.0734 - activation_1_loss: 1.1122 - val_loss: 1.6567 - val_activation_loss: 0.8505 - val_activation_1_loss: 0.8057\nEpoch 2/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.7133 - activation_loss: 0.8573 - activation_1_loss: 0.8560\nEpoch 00002: val_loss improved from 1.65674 to 1.65518, saving model to v0-roberta-3.h5\n21984/21984 [==============================] - 394s 18ms/sample - loss: 1.7144 - activation_loss: 0.8586 - activation_1_loss: 0.8558 - val_loss: 1.6552 - val_activation_loss: 0.8625 - val_activation_1_loss: 0.7922\nEpoch 3/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.5799 - activation_loss: 0.7943 - activation_1_loss: 0.7857\nEpoch 00003: val_loss did not improve from 1.65518\n21984/21984 [==============================] - 393s 18ms/sample - loss: 1.5805 - activation_loss: 0.7947 - activation_1_loss: 0.7857 - val_loss: 1.6800 - val_activation_loss: 0.8403 - val_activation_1_loss: 0.8391\nLoading model...\nPredicting OOF...\n5496/5496 [==============================] - 35s 6ms/sample\n>>>> FOLD 4 Jaccard score = 0.7058771714558217\nTraining FOLD 5:\nTrain on 21984 samples, validate on 5496 samples\nEpoch 1/3\n21952/21984 [============================>.] - ETA: 0s - loss: 2.2046 - activation_loss: 1.1123 - activation_1_loss: 1.0923\nEpoch 00001: val_loss improved from inf to 1.67776, saving model to v0-roberta-4.h5\n21984/21984 [==============================] - 408s 19ms/sample - loss: 2.2039 - activation_loss: 1.1118 - activation_1_loss: 1.0921 - val_loss: 1.6778 - val_activation_loss: 0.8853 - val_activation_1_loss: 0.7935\nEpoch 2/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.6340 - activation_loss: 0.8414 - activation_1_loss: 0.7927\nEpoch 00002: val_loss improved from 1.67776 to 1.64511, saving model to v0-roberta-4.h5\n21984/21984 [==============================] - 394s 18ms/sample - loss: 1.6343 - activation_loss: 0.8417 - activation_1_loss: 0.7926 - val_loss: 1.6451 - val_activation_loss: 0.8537 - val_activation_1_loss: 0.7922\nEpoch 3/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.4890 - activation_loss: 0.7667 - activation_1_loss: 0.7223\nEpoch 00003: val_loss did not improve from 1.64511\n21984/21984 [==============================] - 392s 18ms/sample - loss: 1.4888 - activation_loss: 0.7663 - activation_1_loss: 0.7225 - val_loss: 1.6655 - val_activation_loss: 0.8540 - val_activation_1_loss: 0.8124\nLoading model...\nPredicting OOF...\n5496/5496 [==============================] - 36s 7ms/sample\n>>>> FOLD 5 Jaccard score = 0.7068238471022878\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['selected_text'] = QA_model.predict(pred_df=test, \n                                         input_ids=test_input_ids, \n                                         attention_mask=test_attention_mask, \n                                         token_type_ids=test_token_type_ids, \n                                         n_models=5)","execution_count":22,"outputs":[{"output_type":"stream","text":"Loading model...\n3534/3534 [==============================] - 23s 7ms/sample\nLoading model...\n3534/3534 [==============================] - 24s 7ms/sample\nLoading model...\n3534/3534 [==============================] - 24s 7ms/sample\nLoading model...\n3534/3534 [==============================] - 24s 7ms/sample\nLoading model...\n3534/3534 [==============================] - 24s 7ms/sample\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['textID','selected_text']].to_csv('submission.csv',index=False)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.sample(25)","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"          textID                                               text sentiment  \\\n2094  d60072b03a  He can`t fix it.   I guess I`ll write until I ...  negative   \n1176  3cb4d10927           Watching WALL-E.....it`s so cute but sad   neutral   \n2774  5fb30f858b   Probably not, kinda expensive and we have to ...  negative   \n480   819626535b   Glad to hear you made it out, I hear that pla...  positive   \n3189  1a857a38fb  Weekend is getting close. Too bad I`ll be stuc...  positive   \n2246  676b733c57  Buying pretty shiny beads and things  I feel q...   neutral   \n2418  49c713c76d         Is It The Bit Where Hollie Started Crying?   neutral   \n1184  d2141d6d47                                        Thats great  positive   \n907   b3fa6e5c24   I always hope they will die out but then i se...  negative   \n2181  8f851c59f6   - halla!!! doing ok- got a cold but trying to...   neutral   \n1562  49ec0f5742   ... thx for ur msg, so awesome!  luv the new ...  positive   \n401   97e53162ff                                  dear oh dear.....  negative   \n2718  ca8ae20678  valium makes you feel goood. i need more. i ca...   neutral   \n1393  c281b14817  last class at 10:30. One final tomorrow and 2 ...   neutral   \n1848  c9dfbf4275   I have a new found respect for you now that I...   neutral   \n2119  91c44d5912                             What`s all your fault?   neutral   \n1911  81defce8a3    http://twitter.com/friends?page=20 press pre...   neutral   \n2921  04c455e820  Does `Real Detroit Weekly` not have a website....  negative   \n1740  48a90031bd  got a sniffle, got the kids and hubby just lef...   neutral   \n635   d9e1b10a4f  wow.. tomorrow and then it`s over. i`ll never ...  negative   \n1808  932af12853  Hey! Let`s Follow each other! Wouldn`t that ju...  positive   \n2076  389a3e653c   no problem! i think it`s a great thing to ref...  positive   \n2538  6dc1aa9db8   hey Gerardo! (late response) that day i was t...  negative   \n2963  08ad89c6b0  Hi .. I have the net YAYY.. Im here for a shor...  positive   \n307   90be8eaffb  I want a new phone  I`ve seen too much cellpho...  negative   \n\n                                          selected_text  \n2094                                              lame.  \n1176           watching wall-e.....it`s so cute but sad  \n2774                                          expensive  \n480                                                glad  \n3189   hopefully i`ll be able to get out next weeken...  \n2246   buying pretty shiny beads and things i feel q...  \n2418         is it the bit where hollie started crying?  \n1184                                        thats great  \n907                                                 sad  \n2181   - halla!!! doing ok- got a cold but trying to...  \n1562                                        so awesome!  \n401                                   dear oh dear.....  \n2718   valium makes you feel goood. i need more. i c...  \n1393   last class at 10:30. one final tomorrow and 2...  \n1848   i have a new found respect for you now that i...  \n2119                             what`s all your fault?  \n1911   http://twitter.com/friends?page=20 press prev...  \n2921                          oh the horror, the horror  \n1740   got a sniffle, got the kids and hubby just le...  \n635                                   it`s kind of sad.  \n1808                                          awesome!?  \n2076                                              great  \n2538                                              upset  \n2963                                             yayy..  \n307            i`ve seen too much cellphone commercials  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>sentiment</th>\n      <th>selected_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2094</th>\n      <td>d60072b03a</td>\n      <td>He can`t fix it.   I guess I`ll write until I ...</td>\n      <td>negative</td>\n      <td>lame.</td>\n    </tr>\n    <tr>\n      <th>1176</th>\n      <td>3cb4d10927</td>\n      <td>Watching WALL-E.....it`s so cute but sad</td>\n      <td>neutral</td>\n      <td>watching wall-e.....it`s so cute but sad</td>\n    </tr>\n    <tr>\n      <th>2774</th>\n      <td>5fb30f858b</td>\n      <td>Probably not, kinda expensive and we have to ...</td>\n      <td>negative</td>\n      <td>expensive</td>\n    </tr>\n    <tr>\n      <th>480</th>\n      <td>819626535b</td>\n      <td>Glad to hear you made it out, I hear that pla...</td>\n      <td>positive</td>\n      <td>glad</td>\n    </tr>\n    <tr>\n      <th>3189</th>\n      <td>1a857a38fb</td>\n      <td>Weekend is getting close. Too bad I`ll be stuc...</td>\n      <td>positive</td>\n      <td>hopefully i`ll be able to get out next weeken...</td>\n    </tr>\n    <tr>\n      <th>2246</th>\n      <td>676b733c57</td>\n      <td>Buying pretty shiny beads and things  I feel q...</td>\n      <td>neutral</td>\n      <td>buying pretty shiny beads and things i feel q...</td>\n    </tr>\n    <tr>\n      <th>2418</th>\n      <td>49c713c76d</td>\n      <td>Is It The Bit Where Hollie Started Crying?</td>\n      <td>neutral</td>\n      <td>is it the bit where hollie started crying?</td>\n    </tr>\n    <tr>\n      <th>1184</th>\n      <td>d2141d6d47</td>\n      <td>Thats great</td>\n      <td>positive</td>\n      <td>thats great</td>\n    </tr>\n    <tr>\n      <th>907</th>\n      <td>b3fa6e5c24</td>\n      <td>I always hope they will die out but then i se...</td>\n      <td>negative</td>\n      <td>sad</td>\n    </tr>\n    <tr>\n      <th>2181</th>\n      <td>8f851c59f6</td>\n      <td>- halla!!! doing ok- got a cold but trying to...</td>\n      <td>neutral</td>\n      <td>- halla!!! doing ok- got a cold but trying to...</td>\n    </tr>\n    <tr>\n      <th>1562</th>\n      <td>49ec0f5742</td>\n      <td>... thx for ur msg, so awesome!  luv the new ...</td>\n      <td>positive</td>\n      <td>so awesome!</td>\n    </tr>\n    <tr>\n      <th>401</th>\n      <td>97e53162ff</td>\n      <td>dear oh dear.....</td>\n      <td>negative</td>\n      <td>dear oh dear.....</td>\n    </tr>\n    <tr>\n      <th>2718</th>\n      <td>ca8ae20678</td>\n      <td>valium makes you feel goood. i need more. i ca...</td>\n      <td>neutral</td>\n      <td>valium makes you feel goood. i need more. i c...</td>\n    </tr>\n    <tr>\n      <th>1393</th>\n      <td>c281b14817</td>\n      <td>last class at 10:30. One final tomorrow and 2 ...</td>\n      <td>neutral</td>\n      <td>last class at 10:30. one final tomorrow and 2...</td>\n    </tr>\n    <tr>\n      <th>1848</th>\n      <td>c9dfbf4275</td>\n      <td>I have a new found respect for you now that I...</td>\n      <td>neutral</td>\n      <td>i have a new found respect for you now that i...</td>\n    </tr>\n    <tr>\n      <th>2119</th>\n      <td>91c44d5912</td>\n      <td>What`s all your fault?</td>\n      <td>neutral</td>\n      <td>what`s all your fault?</td>\n    </tr>\n    <tr>\n      <th>1911</th>\n      <td>81defce8a3</td>\n      <td>http://twitter.com/friends?page=20 press pre...</td>\n      <td>neutral</td>\n      <td>http://twitter.com/friends?page=20 press prev...</td>\n    </tr>\n    <tr>\n      <th>2921</th>\n      <td>04c455e820</td>\n      <td>Does `Real Detroit Weekly` not have a website....</td>\n      <td>negative</td>\n      <td>oh the horror, the horror</td>\n    </tr>\n    <tr>\n      <th>1740</th>\n      <td>48a90031bd</td>\n      <td>got a sniffle, got the kids and hubby just lef...</td>\n      <td>neutral</td>\n      <td>got a sniffle, got the kids and hubby just le...</td>\n    </tr>\n    <tr>\n      <th>635</th>\n      <td>d9e1b10a4f</td>\n      <td>wow.. tomorrow and then it`s over. i`ll never ...</td>\n      <td>negative</td>\n      <td>it`s kind of sad.</td>\n    </tr>\n    <tr>\n      <th>1808</th>\n      <td>932af12853</td>\n      <td>Hey! Let`s Follow each other! Wouldn`t that ju...</td>\n      <td>positive</td>\n      <td>awesome!?</td>\n    </tr>\n    <tr>\n      <th>2076</th>\n      <td>389a3e653c</td>\n      <td>no problem! i think it`s a great thing to ref...</td>\n      <td>positive</td>\n      <td>great</td>\n    </tr>\n    <tr>\n      <th>2538</th>\n      <td>6dc1aa9db8</td>\n      <td>hey Gerardo! (late response) that day i was t...</td>\n      <td>negative</td>\n      <td>upset</td>\n    </tr>\n    <tr>\n      <th>2963</th>\n      <td>08ad89c6b0</td>\n      <td>Hi .. I have the net YAYY.. Im here for a shor...</td>\n      <td>positive</td>\n      <td>yayy..</td>\n    </tr>\n    <tr>\n      <th>307</th>\n      <td>90be8eaffb</td>\n      <td>I want a new phone  I`ve seen too much cellpho...</td>\n      <td>negative</td>\n      <td>i`ve seen too much cellphone commercials</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}