{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Import libraries\nimport os\nimport re\nimport string\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tokenizers\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/tf-roberta/pretrained-roberta-base.h5\n/kaggle/input/tf-roberta/merges-roberta-base.txt\n/kaggle/input/tf-roberta/config-roberta-base.json\n/kaggle/input/tf-roberta/vocab-roberta-base.json\n/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n/kaggle/input/tweet-sentiment-extraction/test.csv\n/kaggle/input/tweet-sentiment-extraction/train.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-processing and Transformation"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training set has {} data points\".format(len(train)))\nprint(\"Testing set has {} data points\".format(len(test)))","execution_count":4,"outputs":[{"output_type":"stream","text":"Training set has 27481 data points\nTesting set has 3534 data points\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n\n                         selected_text sentiment  \n0  I`d have responded, if I were going   neutral  \n1                             Sooo SAD  negative  \n2                          bullying me  negative  \n3                       leave me alone  negative  \n4                        Sons of ****,  negative  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>549e992a42</td>\n      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n      <td>Sooo SAD</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>088c60f138</td>\n      <td>my boss is bullying me...</td>\n      <td>bullying me</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9642c003ef</td>\n      <td>what interview! leave me alone</td>\n      <td>leave me alone</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>Sons of ****,</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"       textID                                               text sentiment\n0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n3  01082688c6                                        happy bday!  positive\n4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f87dea47db</td>\n      <td>Last session of the day  http://twitpic.com/67ezh</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>96d74cb729</td>\n      <td>Shanghai is also really exciting (precisely -...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eee518ae67</td>\n      <td>Recession hit Veronique Branquinho, she has to...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01082688c6</td>\n      <td>happy bday!</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33987a8ee5</td>\n      <td>http://twitpic.com/4w75p - I like it!!</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Check for NaN values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"textID           0\ntext             1\nselected_text    1\nsentiment        0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isna().sum()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"textID       0\ntext         0\nsentiment    0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since there is only one NaN value, let's drop it\n# train = train.dropna(axis=0).reset_index(drop=True)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training set has {} data points\".format(len(train)))\nprint(\"Testing set has {} data points\".format(len(test)))\ntrain.isna().sum()","execution_count":10,"outputs":[{"output_type":"stream","text":"Training set has 27481 data points\nTesting set has 3534 data points\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"textID           0\ntext             1\nselected_text    1\nsentiment        0\ndtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Removing punctuations & stopwords, or not? "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if punctuation appears in selected_text\nselected_text_has_punctuation = train.selected_text.str.extract(\n                                                        r'([{}]+)'.format(\n                                                            re.escape(\n                                                                string.punctuation)))\n# number of selected_text with punctuations\nselected_text_has_punctuation.isna().sum() ","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"0    10982\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# observing some tweets whose selected_text contain punctuations \ntrain.loc[selected_text_has_punctuation.dropna().index].head()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"        textID                                               text  \\\n0   cb774db0d1                I`d have responded, if I were going   \n4   358bd9e861   Sons of ****, why couldn`t they put them on t...   \n5   28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n9   fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n10  2339a9b08b   as much as i love to be hopeful, i reckon the...   \n\n                                        selected_text sentiment  \n0                 I`d have responded, if I were going   neutral  \n4                                       Sons of ****,  negative  \n5   http://www.dothebouncy.com/smf - some shameles...   neutral  \n9                        Wow... u just became cooler.  positive  \n10  as much as i love to be hopeful, i reckon the ...   neutral  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>textID</th>\n      <th>text</th>\n      <th>selected_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cb774db0d1</td>\n      <td>I`d have responded, if I were going</td>\n      <td>I`d have responded, if I were going</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>358bd9e861</td>\n      <td>Sons of ****, why couldn`t they put them on t...</td>\n      <td>Sons of ****,</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>28b57f3990</td>\n      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>fc2cbefa9d</td>\n      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n      <td>Wow... u just became cooler.</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2339a9b08b</td>\n      <td>as much as i love to be hopeful, i reckon the...</td>\n      <td>as much as i love to be hopeful, i reckon the ...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"- Punctuation seems to appear in quite a lot of our extracted examples. I'll not remove punctuations for this dataset.\n- Also, I need to preserve stopwords as it can be seen in the above `neutral` sentiment that the tweet *text* has been extracted as-is in the *selected_text*. "},{"metadata":{},"cell_type":"markdown","source":"### Deciding max *text* length"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text.str.len().max()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"141.0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 148","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenizer"},{"metadata":{},"cell_type":"markdown","source":"The pretrained RoBERTa model and tokenizer are from huggingface [transformers](https://huggingface.co/transformers/main_classes/model.html?highlight=save_pretrained) library. They can be downloaded by using the `from_pretrained()` method or attached to a kaggle kerned from [here](https://www.kaggle.com/cdeotte/tf-roberta)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, data_df, tokenizer, train=True, max_len=96):\n        self.data = data_df.dropna(axis=0).reset_index(drop=True)\n        self.is_train = True if train else False\n        self.sentiment_tokens = {\n            'positive': tokenizer.encode('positive').ids[0], \n            'negative': tokenizer.encode('negative').ids[0],\n            'neutral': tokenizer.encode('neutral').ids[0]\n        }\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def ByteLevelBPEPreprocessor(self, text, selected_text, sentiment):\n        \n        text = \" \" + \" \".join(text.split())\n        enc = self.tokenizer.encode(text)\n        s_tok = self.sentiment_tokens[sentiment]\n        \n        # Get InputIDs\n        input_ids = np.ones((self.max_len),\n                            dtype = 'int32')\n        input_ids[:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n\n        # Get Attention mask\n        attention_mask = np.zeros((self.max_len),\n                                  dtype='int32')\n        attention_mask[:len(enc.ids)+5] = 1\n        \n        if self.is_train:\n            selected_text = \" \".join(selected_text.split())\n            idx = text.find(selected_text)\n            char_tokens = np.zeros((len(text)))\n            char_tokens[idx:idx+len(selected_text)] = 1\n            # if text has ' ' prefix\n            if text[idx-1] == ' ': \n                char_tokens[idx-1] = 1\n                \n            # Get start and end token for selected_text in input IDs\n            start_tokens = np.zeros((self.max_len),\n                                    dtype='int32')\n            end_tokens = np.zeros((self.max_len),\n                                  dtype='int32')\n            ptr_idx = 0\n            label_idx = list()\n            for i, enc_id in enumerate(enc.ids):\n                sub_word = self.tokenizer.decode([enc_id])\n                if sum(char_tokens[ptr_idx:ptr_idx+len(sub_word)]) > 0:\n                    label_idx.append(i)\n                ptr_idx += len(sub_word)\n            if label_idx:\n                # + 1 as we added prefix before\n                start_tokens[label_idx[0] + 1] = 1\n                end_tokens[label_idx[-1] + 1] = 1\n            return input_ids, attention_mask, start_tokens, end_tokens\n        \n        return input_ids, attention_mask\n            \n    def __call__(self):\n        data_len = len(self.data)\n        input_ids = np.ones((data_len, self.max_len), \n                            dtype='int32')\n        attention_mask = np.zeros((data_len, self.max_len), \n                                  dtype='int32')\n        token_type_ids = np.zeros((data_len, self.max_len),\n                                  dtype='int32')\n        if self.is_train:\n            start_tokens = np.zeros((data_len, self.max_len),\n                                    dtype='int32')\n            end_tokens = np.zeros((data_len, self.max_len),\n                                  dtype='int32')\n        for i, row in tqdm(self.data.iterrows(), total=len(self.data)):\n            out = self.ByteLevelBPEPreprocessor(\n                row['text'], \n                row['selected_text'] if self.is_train else None, \n                row['sentiment']\n            )\n            if self.is_train:\n                input_ids[i], attention_mask[i], start_tokens[i], end_tokens[i] = out\n            else:\n                input_ids[i], attention_mask[i] = out\n        if self.is_train:\n            return input_ids, attention_mask, token_type_ids, start_tokens, end_tokens\n        return input_ids, attention_mask, token_type_ids","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformerQA:\n    def __init__(self, max_len, model_path, tokenizer, fit=True):\n        self.max_len = max_len\n        self.model_path = model_path\n        self.tokenizer = tokenizer\n        \n    def roberta_model(self):\n        input_ids = tf.keras.layers.Input((self.max_len,),\n                                          dtype=tf.int32)\n        attention_mask = tf.keras.layers.Input((self.max_len,),\n                                               dtype=tf.int32)\n        token_type_ids = tf.keras.layers.Input((self.max_len,),\n                                               dtype=tf.int32)\n\n        config = RobertaConfig.from_pretrained(\n            os.path.join(self.model_path, 'config-roberta-base.json')\n        )\n        roberta_model = TFRobertaModel.from_pretrained(\n            os.path.join(self.model_path, 'pretrained-roberta-base.h5'),\n            config=config\n        )\n        x = roberta_model(inputs=input_ids,\n                          attention_mask=attention_mask,\n                          token_type_ids=token_type_ids)\n\n        x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n        x1 = tf.keras.layers.Conv1D(1,1)(x1)\n        x1 = tf.keras.layers.Flatten()(x1)\n        x1 = tf.keras.layers.Activation('softmax')(x1)\n\n        x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n        x2 = tf.keras.layers.Conv1D(1,1)(x2)\n        x2 = tf.keras.layers.Flatten()(x2)\n        x2 = tf.keras.layers.Activation('softmax')(x2)\n\n        model = tf.keras.models.Model(\n            inputs=[input_ids, attention_mask, token_type_ids], \n            outputs=[x1,x2]\n        )\n        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n        model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n        return model\n    \n    def jaccard(self, str1, str2):\n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        if (len(a)==0) & (len(b)==0): return 0.5\n        c = a.intersection(b)\n        return float(len(c)) / (len(a) + len(b) - len(c))\n    \n    def get_model_selected_text(self, data_df, preds_start, preds_end):\n        st_list = []\n        for k in range(len(data_df)):\n            idx_start = np.argmax(preds_start[k,])\n            idx_end = np.argmax(preds_end[k,])\n            if idx_start > idx_end:\n                st = data_df.loc[k,'text']\n#                 if data_df.loc[k, 'sentiment'] != 'neutral':\n#                     st = st.split()[idx_start]\n            else:\n                text = \" \" + \" \".join(data_df.loc[k,'text'].split())\n                enc = self.tokenizer.encode(text)\n                st = self.tokenizer.decode(enc.ids[idx_start-1:idx_end])\n            st_list.append(st)\n        return st_list\n    \n    def fit(self, train_df, input_ids, attention_mask, \n            token_type_ids, start_tokens, end_tokens, \n            stratify_y, VER='v0', verbose=1):\n        avg_score = []\n        oof_start = np.zeros((input_ids.shape[0],\n                              self.max_len))\n        oof_end = np.zeros((input_ids.shape[0],\n                            self.max_len))\n        skf = StratifiedKFold(n_splits=5,\n                              shuffle=True,\n                              random_state=42)\n        for fold, (idxT,idxV) in enumerate(skf.split(input_ids,\n                                                     stratify_y)):\n            print('Training FOLD {}:'.format(fold+1))\n            K.clear_session()\n            model = self.roberta_model()\n            sv = tf.keras.callbacks.ModelCheckpoint(\n                '{}-roberta-{}.h5'.format(VER, fold), \n                monitor='val_loss', \n                verbose=verbose, \n                save_best_only=True,\n                save_weights_only=True, \n                mode='auto', \n                save_freq='epoch'\n            )\n            model.fit([input_ids[idxT,], \n                       attention_mask[idxT,], \n                       token_type_ids[idxT,]], \n                      [start_tokens[idxT,], end_tokens[idxT,]],\n                      epochs=3, \n                      batch_size=32, \n                      verbose=verbose, \n                      callbacks=[sv],\n                      validation_data=(\n                          [\n                              input_ids[idxV,],\n                              attention_mask[idxV,],\n                              token_type_ids[idxV,]\n                          ], \n                          [start_tokens[idxV,], end_tokens[idxV,]]\n                      )\n                     )\n            # Load best saved model from disk\n            print('Loading model...')\n            model.load_weights('{}-roberta-{}.h5'.format(VER, fold))\n            \n            # Predicting OOF samples\n            print('Predicting OOF...')\n            oof_start[idxV,],oof_end[idxV,] = model.predict(\n                [\n                    input_ids[idxV,],\n                    attention_mask[idxV,],\n                    token_type_ids[idxV,]\n                ],\n                verbose=verbose\n            )\n            \n            pred_df = train_df.loc[idxV].reset_index(drop=True)\n            pred_df['oof_st'] = self.get_model_selected_text(\n                data_df=pred_df,\n                preds_start=oof_start[idxV,],\n                preds_end=oof_end[idxV,]\n            )\n            fold_val_score = pred_df.apply(\n                lambda x: self.jaccard(x['selected_text'], \n                                       x['oof_st']\n                                      ),\n                axis=1\n            ).mean()\n            avg_score.append(fold_val_score)\n            print('>>>> FOLD {} Jaccard score = {}'.format(fold+1, \n                                                           fold_val_score))\n    def predict(self, pred_df, input_ids, attention_mask, \n                token_type_ids, n_models, VER='v0', verbose=1):\n        preds_start = np.zeros((input_ids.shape[0],\n                                self.max_len))\n        preds_end = np.zeros((input_ids.shape[0],\n                              self.max_len))\n        for i in range(n_models):\n            K.clear_session()\n            model = self.roberta_model()\n            \n            print('Loading model...')\n            model.load_weights('{}-roberta-{}.h5'.format(VER, i))\n            \n            preds = model.predict(\n                [input_ids, attention_mask, token_type_ids],\n                verbose=verbose\n            )\n            preds_start += preds[0]/n_models\n            preds_end += preds[1]/n_models\n        \n        test_st = self.get_model_selected_text(\n            data_df=pred_df,\n            preds_start=preds_start,\n            preds_end=preds_end\n        )\n        return test_st","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = TweetDataset(train, tokenizer, train=True, max_len=MAX_LEN)\ninput_ids, attention_mask, token_type_ids, start_tokens, end_tokens = train_data()","execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=27480.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f7999f119b49929f91b0fba3e8d073"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = TweetDataset(test, tokenizer, train=False, max_len=MAX_LEN)\ntest_input_ids, test_attention_mask, test_token_type_ids = test_data()","execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=3534.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21ec5a5c7f694db0b671302492b00f31"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"QA_model = TransformerQA(max_len=MAX_LEN, \n                         model_path=PATH, \n                         tokenizer=tokenizer)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"QA_model.fit(train_df=train_data.data, \n             input_ids=input_ids, \n             attention_mask=attention_mask, \n             token_type_ids=token_type_ids, \n             start_tokens=start_tokens, \n             end_tokens=end_tokens, \n             stratify_y=train_data.data.sentiment.values)","execution_count":null,"outputs":[{"output_type":"stream","text":"Training FOLD 1:\nTrain on 21984 samples, validate on 5496 samples\nEpoch 1/3\n21952/21984 [============================>.] - ETA: 0s - loss: 2.2045 - activation_loss: 1.0921 - activation_1_loss: 1.1124\nEpoch 00001: val_loss improved from inf to 1.75551, saving model to v0-roberta-0.h5\n21984/21984 [==============================] - 411s 19ms/sample - loss: 2.2034 - activation_loss: 1.0917 - activation_1_loss: 1.1118 - val_loss: 1.7555 - val_activation_loss: 0.9032 - val_activation_1_loss: 0.8518\nEpoch 2/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.6337 - activation_loss: 0.8407 - activation_1_loss: 0.7930\nEpoch 00002: val_loss improved from 1.75551 to 1.65791, saving model to v0-roberta-0.h5\n21984/21984 [==============================] - 393s 18ms/sample - loss: 1.6339 - activation_loss: 0.8408 - activation_1_loss: 0.7931 - val_loss: 1.6579 - val_activation_loss: 0.8442 - val_activation_1_loss: 0.8132\nEpoch 3/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.4827 - activation_loss: 0.7651 - activation_1_loss: 0.7176\nEpoch 00003: val_loss did not improve from 1.65791\n21984/21984 [==============================] - 393s 18ms/sample - loss: 1.4830 - activation_loss: 0.7651 - activation_1_loss: 0.7180 - val_loss: 1.7415 - val_activation_loss: 0.8789 - val_activation_1_loss: 0.8620\nLoading model...\nPredicting OOF...\n5496/5496 [==============================] - 36s 7ms/sample\n>>>> FOLD 1 Jaccard score = 0.7031862155715445\nTraining FOLD 2:\nTrain on 21984 samples, validate on 5496 samples\nEpoch 1/3\n21952/21984 [============================>.] - ETA: 0s - loss: 2.0852 - activation_loss: 1.0573 - activation_1_loss: 1.0279\nEpoch 00001: val_loss improved from inf to 1.62839, saving model to v0-roberta-1.h5\n21984/21984 [==============================] - 410s 19ms/sample - loss: 2.0841 - activation_loss: 1.0570 - activation_1_loss: 1.0271 - val_loss: 1.6284 - val_activation_loss: 0.8578 - val_activation_1_loss: 0.7697\nEpoch 2/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.6747 - activation_loss: 0.8455 - activation_1_loss: 0.8292\nEpoch 00002: val_loss did not improve from 1.62839\n21984/21984 [==============================] - 393s 18ms/sample - loss: 1.6744 - activation_loss: 0.8457 - activation_1_loss: 0.8287 - val_loss: 1.6350 - val_activation_loss: 0.8460 - val_activation_1_loss: 0.7882\nEpoch 3/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.5038 - activation_loss: 0.7771 - activation_1_loss: 0.7267\nEpoch 00003: val_loss improved from 1.62839 to 1.59848, saving model to v0-roberta-1.h5\n21984/21984 [==============================] - 394s 18ms/sample - loss: 1.5044 - activation_loss: 0.7770 - activation_1_loss: 0.7274 - val_loss: 1.5985 - val_activation_loss: 0.8259 - val_activation_1_loss: 0.7717\nLoading model...\nPredicting OOF...\n5496/5496 [==============================] - 36s 7ms/sample\n>>>> FOLD 2 Jaccard score = 0.707532562130843\nTraining FOLD 3:\nTrain on 21984 samples, validate on 5496 samples\nEpoch 1/3\n21952/21984 [============================>.] - ETA: 0s - loss: 2.1752 - activation_loss: 1.0728 - activation_1_loss: 1.1024\nEpoch 00001: val_loss improved from inf to 1.67778, saving model to v0-roberta-2.h5\n21984/21984 [==============================] - 408s 19ms/sample - loss: 2.1758 - activation_loss: 1.0736 - activation_1_loss: 1.1022 - val_loss: 1.6778 - val_activation_loss: 0.8559 - val_activation_1_loss: 0.8217\nEpoch 2/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.6938 - activation_loss: 0.8661 - activation_1_loss: 0.8278\nEpoch 00002: val_loss improved from 1.67778 to 1.66205, saving model to v0-roberta-2.h5\n21984/21984 [==============================] - 394s 18ms/sample - loss: 1.6932 - activation_loss: 0.8656 - activation_1_loss: 0.8276 - val_loss: 1.6620 - val_activation_loss: 0.8450 - val_activation_1_loss: 0.8171\nEpoch 3/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.5881 - activation_loss: 0.8095 - activation_1_loss: 0.7786\nEpoch 00003: val_loss improved from 1.66205 to 1.61364, saving model to v0-roberta-2.h5\n21984/21984 [==============================] - 394s 18ms/sample - loss: 1.5880 - activation_loss: 0.8091 - activation_1_loss: 0.7789 - val_loss: 1.6136 - val_activation_loss: 0.8342 - val_activation_1_loss: 0.7792\nLoading model...\nPredicting OOF...\n5496/5496 [==============================] - 36s 7ms/sample\n>>>> FOLD 3 Jaccard score = 0.7096439841535017\nTraining FOLD 4:\nTrain on 21984 samples, validate on 5496 samples\nEpoch 1/3\n21952/21984 [============================>.] - ETA: 0s - loss: 2.1854 - activation_loss: 1.0734 - activation_1_loss: 1.1120\nEpoch 00001: val_loss improved from inf to 1.65674, saving model to v0-roberta-3.h5\n21984/21984 [==============================] - 407s 19ms/sample - loss: 2.1856 - activation_loss: 1.0734 - activation_1_loss: 1.1122 - val_loss: 1.6567 - val_activation_loss: 0.8505 - val_activation_1_loss: 0.8057\nEpoch 2/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.7133 - activation_loss: 0.8573 - activation_1_loss: 0.8560\nEpoch 00002: val_loss improved from 1.65674 to 1.65518, saving model to v0-roberta-3.h5\n21984/21984 [==============================] - 394s 18ms/sample - loss: 1.7144 - activation_loss: 0.8586 - activation_1_loss: 0.8558 - val_loss: 1.6552 - val_activation_loss: 0.8625 - val_activation_1_loss: 0.7922\nEpoch 3/3\n21952/21984 [============================>.] - ETA: 0s - loss: 1.5799 - activation_loss: 0.7943 - activation_1_loss: 0.7857\nEpoch 00003: val_loss did not improve from 1.65518\n21984/21984 [==============================] - 393s 18ms/sample - loss: 1.5805 - activation_loss: 0.7947 - activation_1_loss: 0.7857 - val_loss: 1.6800 - val_activation_loss: 0.8403 - val_activation_1_loss: 0.8391\nLoading model...\nPredicting OOF...\n5496/5496 [==============================] - 35s 6ms/sample\n>>>> FOLD 4 Jaccard score = 0.7058771714558217\nTraining FOLD 5:\nTrain on 21984 samples, validate on 5496 samples\nEpoch 1/3\n13088/21984 [================>.............] - ETA: 2:33 - loss: 2.4826 - activation_loss: 1.2447 - activation_1_loss: 1.2379","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['selected_text'] = QA_model.predict(pred_df=test, \n                                         input_ids=test_input_ids, \n                                         attention_mask=test_attention_mask, \n                                         token_type_ids=test_token_type_ids, \n                                         n_models=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['textID','selected_text']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.sample(25)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}