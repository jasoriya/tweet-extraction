{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tokenizers\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tf-roberta/pretrained-roberta-base.h5\n",
      "/kaggle/input/tf-roberta/merges-roberta-base.txt\n",
      "/kaggle/input/tf-roberta/config-roberta-base.json\n",
      "/kaggle/input/tf-roberta/vocab-roberta-base.json\n",
      "/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n",
      "/kaggle/input/tweet-sentiment-extraction/test.csv\n",
      "/kaggle/input/tweet-sentiment-extraction/train.csv\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 27481 data points\n",
      "Testing set has 3534 data points\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set has {} data points\".format(len(train)))\n",
    "print(\"Testing set has {} data points\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n",
       "3  01082688c6                                        happy bday!  positive\n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textID           0\n",
       "text             1\n",
       "selected_text    1\n",
       "sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textID       0\n",
       "text         0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is only one NaN value, let's drop it\n",
    "\n",
    "# Dropping it in the TweetDataset class below\n",
    "# train = train.dropna(axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 27481 data points\n",
      "Testing set has 3534 data points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "textID           0\n",
       "text             1\n",
       "selected_text    1\n",
       "sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training set has {} data points\".format(len(train)))\n",
    "print(\"Testing set has {} data points\".format(len(test)))\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuations & stopwords, or not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10982\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if punctuation appears in selected_text\n",
    "selected_text_has_punctuation = train.selected_text.str.extract(\n",
    "                                                        r'([{}]+)'.format(\n",
    "                                                            re.escape(\n",
    "                                                                string.punctuation)))\n",
    "# number of selected_text with punctuations\n",
    "selected_text_has_punctuation.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2339a9b08b</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        textID                                               text  \\\n",
       "0   cb774db0d1                I`d have responded, if I were going   \n",
       "4   358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5   28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "9   fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "10  2339a9b08b   as much as i love to be hopeful, i reckon the...   \n",
       "\n",
       "                                        selected_text sentiment  \n",
       "0                 I`d have responded, if I were going   neutral  \n",
       "4                                       Sons of ****,  negative  \n",
       "5   http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "9                        Wow... u just became cooler.  positive  \n",
       "10  as much as i love to be hopeful, i reckon the ...   neutral  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observing some tweets whose selected_text contain punctuations \n",
    "train.loc[selected_text_has_punctuation.dropna().index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Punctuation seems to appear in quite a lot of our extracted examples. I'll not remove punctuations for this dataset.\n",
    "- Also, I need to preserve stopwords as it can be seen in the above `neutral` sentiment that the tweet *text* has been extracted as-is in the *selected_text*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding max *text* length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text.str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained RoBERTa model and tokenizer are from huggingface [transformers](https://huggingface.co/transformers/main_classes/model.html?highlight=save_pretrained) library. They can be downloaded by using the `from_pretrained()` method or attached to a kaggle kerned from [here](https://www.kaggle.com/cdeotte/tf-roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset:\n",
    "    def __init__(self, data_df, tokenizer, train=True, max_len=96):\n",
    "        self.data = data_df.dropna(axis=0).reset_index(drop=True)\n",
    "        self.is_train = True if train else False\n",
    "        self.sentiment_tokens = {\n",
    "            'positive': tokenizer.encode('positive').ids[0], \n",
    "            'negative': tokenizer.encode('negative').ids[0],\n",
    "            'neutral': tokenizer.encode('neutral').ids[0]\n",
    "        }\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def ByteLevelBPEPreprocessor(self, text, selected_text, sentiment):\n",
    "        \"\"\"Return Input IDs, Attention Mask, Start/End tokens\n",
    "        \n",
    "        This function returns Input IDs and Attention Mask. If this is\n",
    "        training dataset it also return start and end tokens.\n",
    "        \"\"\"\n",
    "        text = \" \" + \" \".join(text.split())\n",
    "        enc = self.tokenizer.encode(text)\n",
    "        s_tok = self.sentiment_tokens[sentiment]\n",
    "        \n",
    "        # Get InputIDs\n",
    "        input_ids = np.ones((self.max_len),\n",
    "                            dtype = 'int32')\n",
    "        input_ids[:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "\n",
    "        # Get Attention mask\n",
    "        attention_mask = np.zeros((self.max_len),\n",
    "                                  dtype='int32')\n",
    "        attention_mask[:len(enc.ids)+5] = 1\n",
    "        \n",
    "        if self.is_train:\n",
    "            selected_text = \" \".join(selected_text.split())\n",
    "            idx = text.find(selected_text)\n",
    "            char_tokens = np.zeros((len(text)))\n",
    "            char_tokens[idx:idx+len(selected_text)] = 1\n",
    "            # if text has ' ' prefix\n",
    "            if text[idx-1] == ' ': \n",
    "                char_tokens[idx-1] = 1\n",
    "                \n",
    "            # Get start and end token for selected_text in input IDs\n",
    "            start_tokens = np.zeros((self.max_len),\n",
    "                                    dtype='int32')\n",
    "            end_tokens = np.zeros((self.max_len),\n",
    "                                  dtype='int32')\n",
    "            ptr_idx = 0\n",
    "            label_idx = list()\n",
    "            for i, enc_id in enumerate(enc.ids):\n",
    "                sub_word = self.tokenizer.decode([enc_id])\n",
    "                if sum(char_tokens[ptr_idx:ptr_idx+len(sub_word)]) > 0:\n",
    "                    label_idx.append(i)\n",
    "                ptr_idx += len(sub_word)\n",
    "            if label_idx:\n",
    "                # + 1 as we added prefix before\n",
    "                start_tokens[label_idx[0] + 1] = 1\n",
    "                end_tokens[label_idx[-1] + 1] = 1\n",
    "            return input_ids, attention_mask, start_tokens, end_tokens\n",
    "        \n",
    "        return input_ids, attention_mask\n",
    "            \n",
    "    def __call__(self):\n",
    "        data_len = len(self.data)\n",
    "        input_ids = np.ones((data_len, self.max_len), \n",
    "                            dtype='int32')\n",
    "        attention_mask = np.zeros((data_len, self.max_len), \n",
    "                                  dtype='int32')\n",
    "        token_type_ids = np.zeros((data_len, self.max_len),\n",
    "                                  dtype='int32')\n",
    "        if self.is_train:\n",
    "            start_tokens = np.zeros((data_len, self.max_len),\n",
    "                                    dtype='int32')\n",
    "            end_tokens = np.zeros((data_len, self.max_len),\n",
    "                                  dtype='int32')\n",
    "        for i, row in tqdm(self.data.iterrows(), total=len(self.data)):\n",
    "            out = self.ByteLevelBPEPreprocessor(\n",
    "                row['text'], \n",
    "                row['selected_text'] if self.is_train else None, \n",
    "                row['sentiment']\n",
    "            )\n",
    "            if self.is_train:\n",
    "                input_ids[i], attention_mask[i], start_tokens[i], end_tokens[i] = out\n",
    "            else:\n",
    "                input_ids[i], attention_mask[i] = out\n",
    "        if self.is_train:\n",
    "            return input_ids, attention_mask, token_type_ids, start_tokens, end_tokens\n",
    "        return input_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerQA:\n",
    "    def __init__(self, max_len, model_path, tokenizer, fit=True):\n",
    "        self.max_len = max_len\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def roberta_model(self):\n",
    "        \"\"\"Return RoBERTa base mode with a custom question answering head\n",
    "        \"\"\"\n",
    "        input_ids = tf.keras.layers.Input((self.max_len,),\n",
    "                                          dtype=tf.int32)\n",
    "        attention_mask = tf.keras.layers.Input((self.max_len,),\n",
    "                                               dtype=tf.int32)\n",
    "        token_type_ids = tf.keras.layers.Input((self.max_len,),\n",
    "                                               dtype=tf.int32)\n",
    "\n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            os.path.join(self.model_path, 'config-roberta-base.json')\n",
    "        )\n",
    "        roberta_model = TFRobertaModel.from_pretrained(\n",
    "            os.path.join(self.model_path, 'pretrained-roberta-base.h5'),\n",
    "            config=config\n",
    "        )\n",
    "        x = roberta_model(inputs=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids)\n",
    "\n",
    "        x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "        x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "        x1 = tf.keras.layers.Flatten()(x1)\n",
    "        x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "\n",
    "        x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "        x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "        x2 = tf.keras.layers.Flatten()(x2)\n",
    "        x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "        model = tf.keras.models.Model(\n",
    "            inputs=[input_ids, attention_mask, token_type_ids], \n",
    "            outputs=[x1,x2]\n",
    "        )\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def jaccard(self, str1, str2):\n",
    "        \"\"\"Return Jaccard similarity score betweeen two strings\n",
    "        \"\"\"\n",
    "        a = set(str1.lower().split()) \n",
    "        b = set(str2.lower().split())\n",
    "        if (len(a)==0) & (len(b)==0): return 0.5\n",
    "        c = a.intersection(b)\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "    \n",
    "    def get_model_selected_text(self, data_df, preds_start, preds_end):\n",
    "        \"\"\"Return list of 'selected_text using the predicted start/end tokens'\n",
    "        \"\"\"\n",
    "        st_list = []\n",
    "        for k in range(len(data_df)):\n",
    "            idx_start = np.argmax(preds_start[k,])\n",
    "            idx_end = np.argmax(preds_end[k,])\n",
    "            if idx_start > idx_end:\n",
    "                st = data_df.loc[k,'text']\n",
    "#                 if data_df.loc[k, 'sentiment'] != 'neutral':\n",
    "#                     st = st.split()[idx_start]\n",
    "            else:\n",
    "                text = \" \" + \" \".join(data_df.loc[k,'text'].split())\n",
    "                enc = self.tokenizer.encode(text)\n",
    "                st = self.tokenizer.decode(enc.ids[idx_start-1:idx_end])\n",
    "            st_list.append(st)\n",
    "        return st_list\n",
    "    \n",
    "    def fit(self, train_df, input_ids, attention_mask, \n",
    "            token_type_ids, start_tokens, end_tokens, \n",
    "            stratify_y, VER='v0', verbose=1):\n",
    "        \"\"\"Fit a RoBERTa model with the training dataset\n",
    "        \"\"\"\n",
    "        avg_score = []\n",
    "        oof_start = np.zeros((input_ids.shape[0],\n",
    "                              self.max_len))\n",
    "        oof_end = np.zeros((input_ids.shape[0],\n",
    "                            self.max_len))\n",
    "        skf = StratifiedKFold(n_splits=5,\n",
    "                              shuffle=True,\n",
    "                              random_state=42)\n",
    "        for fold, (idxT,idxV) in enumerate(skf.split(input_ids,\n",
    "                                                     stratify_y)):\n",
    "            print('Training FOLD {}:'.format(fold+1))\n",
    "            K.clear_session()\n",
    "            model = self.roberta_model()\n",
    "            sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "                '{}-roberta-{}.h5'.format(VER, fold), \n",
    "                monitor='val_loss', \n",
    "                verbose=verbose, \n",
    "                save_best_only=True,\n",
    "                save_weights_only=True, \n",
    "                mode='auto', \n",
    "                save_freq='epoch'\n",
    "            )\n",
    "            model.fit([input_ids[idxT,], \n",
    "                       attention_mask[idxT,], \n",
    "                       token_type_ids[idxT,]], \n",
    "                      [start_tokens[idxT,], end_tokens[idxT,]],\n",
    "                      epochs=3, \n",
    "                      batch_size=32, \n",
    "                      verbose=verbose, \n",
    "                      callbacks=[sv],\n",
    "                      validation_data=(\n",
    "                          [\n",
    "                              input_ids[idxV,],\n",
    "                              attention_mask[idxV,],\n",
    "                              token_type_ids[idxV,]\n",
    "                          ], \n",
    "                          [start_tokens[idxV,], end_tokens[idxV,]]\n",
    "                      )\n",
    "                     )\n",
    "            # Load best saved model from disk\n",
    "            print('Loading model...')\n",
    "            model.load_weights('{}-roberta-{}.h5'.format(VER, fold))\n",
    "            \n",
    "            # Predicting OOF samples\n",
    "            print('Predicting OOF...')\n",
    "            oof_start[idxV,],oof_end[idxV,] = model.predict(\n",
    "                [\n",
    "                    input_ids[idxV,],\n",
    "                    attention_mask[idxV,],\n",
    "                    token_type_ids[idxV,]\n",
    "                ],\n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "            pred_df = train_df.loc[idxV].reset_index(drop=True)\n",
    "            pred_df['oof_st'] = self.get_model_selected_text(\n",
    "                data_df=pred_df,\n",
    "                preds_start=oof_start[idxV,],\n",
    "                preds_end=oof_end[idxV,]\n",
    "            )\n",
    "            fold_val_score = pred_df.apply(\n",
    "                lambda x: self.jaccard(x['selected_text'], \n",
    "                                       x['oof_st']\n",
    "                                      ),\n",
    "                axis=1\n",
    "            ).mean()\n",
    "            avg_score.append(fold_val_score)\n",
    "            print('>>>> FOLD {} Jaccard score = {}'.format(fold+1, \n",
    "                                                           fold_val_score))\n",
    "    def predict(self, pred_df, input_ids, attention_mask, \n",
    "                token_type_ids, n_models, VER='v0', verbose=1):\n",
    "        \"\"\"Return a list of predicted 'selected_text' by loading saved models\n",
    "        \"\"\"\n",
    "        preds_start = np.zeros((input_ids.shape[0],\n",
    "                                self.max_len))\n",
    "        preds_end = np.zeros((input_ids.shape[0],\n",
    "                              self.max_len))\n",
    "        for i in range(n_models):\n",
    "            K.clear_session()\n",
    "            model = self.roberta_model()\n",
    "            \n",
    "            print('Loading model...')\n",
    "            model.load_weights('{}-roberta-{}.h5'.format(VER, i))\n",
    "            \n",
    "            preds = model.predict(\n",
    "                [input_ids, attention_mask, token_type_ids],\n",
    "                verbose=verbose\n",
    "            )\n",
    "            preds_start += preds[0]/n_models\n",
    "            preds_end += preds[1]/n_models\n",
    "        \n",
    "        test_st = self.get_model_selected_text(\n",
    "            data_df=pred_df,\n",
    "            preds_start=preds_start,\n",
    "            preds_end=preds_end\n",
    "        )\n",
    "        return test_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f7999f119b49929f91b0fba3e8d073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27480.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get pre-processed & transformed training inputs and labels\n",
    "\n",
    "train_data = TweetDataset(train, tokenizer, train=True, max_len=MAX_LEN)\n",
    "input_ids, attention_mask, token_type_ids, start_tokens, end_tokens = train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ec5a5c7f694db0b671302492b00f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3534.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get pre-processed & transformed testing inputs\n",
    "\n",
    "test_data = TweetDataset(test, tokenizer, train=False, max_len=MAX_LEN)\n",
    "test_input_ids, test_attention_mask, test_token_type_ids = test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_model = TransformerQA(max_len=MAX_LEN, \n",
    "                         model_path=PATH, \n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FOLD 1:\n",
      "Train on 21984 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.2045 - activation_loss: 1.0921 - activation_1_loss: 1.1124\n",
      "Epoch 00001: val_loss improved from inf to 1.75551, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 411s 19ms/sample - loss: 2.2034 - activation_loss: 1.0917 - activation_1_loss: 1.1118 - val_loss: 1.7555 - val_activation_loss: 0.9032 - val_activation_1_loss: 0.8518\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.6337 - activation_loss: 0.8407 - activation_1_loss: 0.7930\n",
      "Epoch 00002: val_loss improved from 1.75551 to 1.65791, saving model to v0-roberta-0.h5\n",
      "21984/21984 [==============================] - 393s 18ms/sample - loss: 1.6339 - activation_loss: 0.8408 - activation_1_loss: 0.7931 - val_loss: 1.6579 - val_activation_loss: 0.8442 - val_activation_1_loss: 0.8132\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.4827 - activation_loss: 0.7651 - activation_1_loss: 0.7176\n",
      "Epoch 00003: val_loss did not improve from 1.65791\n",
      "21984/21984 [==============================] - 393s 18ms/sample - loss: 1.4830 - activation_loss: 0.7651 - activation_1_loss: 0.7180 - val_loss: 1.7415 - val_activation_loss: 0.8789 - val_activation_1_loss: 0.8620\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 36s 7ms/sample\n",
      ">>>> FOLD 1 Jaccard score = 0.7031862155715445\n",
      "Training FOLD 2:\n",
      "Train on 21984 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.0852 - activation_loss: 1.0573 - activation_1_loss: 1.0279\n",
      "Epoch 00001: val_loss improved from inf to 1.62839, saving model to v0-roberta-1.h5\n",
      "21984/21984 [==============================] - 410s 19ms/sample - loss: 2.0841 - activation_loss: 1.0570 - activation_1_loss: 1.0271 - val_loss: 1.6284 - val_activation_loss: 0.8578 - val_activation_1_loss: 0.7697\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.6747 - activation_loss: 0.8455 - activation_1_loss: 0.8292\n",
      "Epoch 00002: val_loss did not improve from 1.62839\n",
      "21984/21984 [==============================] - 393s 18ms/sample - loss: 1.6744 - activation_loss: 0.8457 - activation_1_loss: 0.8287 - val_loss: 1.6350 - val_activation_loss: 0.8460 - val_activation_1_loss: 0.7882\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.5038 - activation_loss: 0.7771 - activation_1_loss: 0.7267\n",
      "Epoch 00003: val_loss improved from 1.62839 to 1.59848, saving model to v0-roberta-1.h5\n",
      "21984/21984 [==============================] - 394s 18ms/sample - loss: 1.5044 - activation_loss: 0.7770 - activation_1_loss: 0.7274 - val_loss: 1.5985 - val_activation_loss: 0.8259 - val_activation_1_loss: 0.7717\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 36s 7ms/sample\n",
      ">>>> FOLD 2 Jaccard score = 0.707532562130843\n",
      "Training FOLD 3:\n",
      "Train on 21984 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.1752 - activation_loss: 1.0728 - activation_1_loss: 1.1024\n",
      "Epoch 00001: val_loss improved from inf to 1.67778, saving model to v0-roberta-2.h5\n",
      "21984/21984 [==============================] - 408s 19ms/sample - loss: 2.1758 - activation_loss: 1.0736 - activation_1_loss: 1.1022 - val_loss: 1.6778 - val_activation_loss: 0.8559 - val_activation_1_loss: 0.8217\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.6938 - activation_loss: 0.8661 - activation_1_loss: 0.8278\n",
      "Epoch 00002: val_loss improved from 1.67778 to 1.66205, saving model to v0-roberta-2.h5\n",
      "21984/21984 [==============================] - 394s 18ms/sample - loss: 1.6932 - activation_loss: 0.8656 - activation_1_loss: 0.8276 - val_loss: 1.6620 - val_activation_loss: 0.8450 - val_activation_1_loss: 0.8171\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.5881 - activation_loss: 0.8095 - activation_1_loss: 0.7786\n",
      "Epoch 00003: val_loss improved from 1.66205 to 1.61364, saving model to v0-roberta-2.h5\n",
      "21984/21984 [==============================] - 394s 18ms/sample - loss: 1.5880 - activation_loss: 0.8091 - activation_1_loss: 0.7789 - val_loss: 1.6136 - val_activation_loss: 0.8342 - val_activation_1_loss: 0.7792\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 36s 7ms/sample\n",
      ">>>> FOLD 3 Jaccard score = 0.7096439841535017\n",
      "Training FOLD 4:\n",
      "Train on 21984 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.1854 - activation_loss: 1.0734 - activation_1_loss: 1.1120\n",
      "Epoch 00001: val_loss improved from inf to 1.65674, saving model to v0-roberta-3.h5\n",
      "21984/21984 [==============================] - 407s 19ms/sample - loss: 2.1856 - activation_loss: 1.0734 - activation_1_loss: 1.1122 - val_loss: 1.6567 - val_activation_loss: 0.8505 - val_activation_1_loss: 0.8057\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.7133 - activation_loss: 0.8573 - activation_1_loss: 0.8560\n",
      "Epoch 00002: val_loss improved from 1.65674 to 1.65518, saving model to v0-roberta-3.h5\n",
      "21984/21984 [==============================] - 394s 18ms/sample - loss: 1.7144 - activation_loss: 0.8586 - activation_1_loss: 0.8558 - val_loss: 1.6552 - val_activation_loss: 0.8625 - val_activation_1_loss: 0.7922\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.5799 - activation_loss: 0.7943 - activation_1_loss: 0.7857\n",
      "Epoch 00003: val_loss did not improve from 1.65518\n",
      "21984/21984 [==============================] - 393s 18ms/sample - loss: 1.5805 - activation_loss: 0.7947 - activation_1_loss: 0.7857 - val_loss: 1.6800 - val_activation_loss: 0.8403 - val_activation_1_loss: 0.8391\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 35s 6ms/sample\n",
      ">>>> FOLD 4 Jaccard score = 0.7058771714558217\n",
      "Training FOLD 5:\n",
      "Train on 21984 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.2046 - activation_loss: 1.1123 - activation_1_loss: 1.0923\n",
      "Epoch 00001: val_loss improved from inf to 1.67776, saving model to v0-roberta-4.h5\n",
      "21984/21984 [==============================] - 408s 19ms/sample - loss: 2.2039 - activation_loss: 1.1118 - activation_1_loss: 1.0921 - val_loss: 1.6778 - val_activation_loss: 0.8853 - val_activation_1_loss: 0.7935\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.6340 - activation_loss: 0.8414 - activation_1_loss: 0.7927\n",
      "Epoch 00002: val_loss improved from 1.67776 to 1.64511, saving model to v0-roberta-4.h5\n",
      "21984/21984 [==============================] - 394s 18ms/sample - loss: 1.6343 - activation_loss: 0.8417 - activation_1_loss: 0.7926 - val_loss: 1.6451 - val_activation_loss: 0.8537 - val_activation_1_loss: 0.7922\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.4890 - activation_loss: 0.7667 - activation_1_loss: 0.7223\n",
      "Epoch 00003: val_loss did not improve from 1.64511\n",
      "21984/21984 [==============================] - 392s 18ms/sample - loss: 1.4888 - activation_loss: 0.7663 - activation_1_loss: 0.7225 - val_loss: 1.6655 - val_activation_loss: 0.8540 - val_activation_1_loss: 0.8124\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 36s 7ms/sample\n",
      ">>>> FOLD 5 Jaccard score = 0.7068238471022878\n"
     ]
    }
   ],
   "source": [
    "# Train the RoBERTa model\n",
    "QA_model.fit(train_df=train_data.data, \n",
    "             input_ids=input_ids, \n",
    "             attention_mask=attention_mask, \n",
    "             token_type_ids=token_type_ids, \n",
    "             start_tokens=start_tokens, \n",
    "             end_tokens=end_tokens, \n",
    "             stratify_y=train_data.data.sentiment.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "3534/3534 [==============================] - 23s 7ms/sample\n",
      "Loading model...\n",
      "3534/3534 [==============================] - 24s 7ms/sample\n",
      "Loading model...\n",
      "3534/3534 [==============================] - 24s 7ms/sample\n",
      "Loading model...\n",
      "3534/3534 [==============================] - 24s 7ms/sample\n",
      "Loading model...\n",
      "3534/3534 [==============================] - 24s 7ms/sample\n"
     ]
    }
   ],
   "source": [
    "# Test the RoBERTa model\n",
    "\n",
    "test['selected_text'] = QA_model.predict(pred_df=test, \n",
    "                                         input_ids=test_input_ids, \n",
    "                                         attention_mask=test_attention_mask, \n",
    "                                         token_type_ids=test_token_type_ids, \n",
    "                                         n_models=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>d60072b03a</td>\n",
       "      <td>He can`t fix it.   I guess I`ll write until I ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>lame.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>3cb4d10927</td>\n",
       "      <td>Watching WALL-E.....it`s so cute but sad</td>\n",
       "      <td>neutral</td>\n",
       "      <td>watching wall-e.....it`s so cute but sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>5fb30f858b</td>\n",
       "      <td>Probably not, kinda expensive and we have to ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>expensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>819626535b</td>\n",
       "      <td>Glad to hear you made it out, I hear that pla...</td>\n",
       "      <td>positive</td>\n",
       "      <td>glad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>1a857a38fb</td>\n",
       "      <td>Weekend is getting close. Too bad I`ll be stuc...</td>\n",
       "      <td>positive</td>\n",
       "      <td>hopefully i`ll be able to get out next weeken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>676b733c57</td>\n",
       "      <td>Buying pretty shiny beads and things  I feel q...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>buying pretty shiny beads and things i feel q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>49c713c76d</td>\n",
       "      <td>Is It The Bit Where Hollie Started Crying?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is it the bit where hollie started crying?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>d2141d6d47</td>\n",
       "      <td>Thats great</td>\n",
       "      <td>positive</td>\n",
       "      <td>thats great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>b3fa6e5c24</td>\n",
       "      <td>I always hope they will die out but then i se...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2181</th>\n",
       "      <td>8f851c59f6</td>\n",
       "      <td>- halla!!! doing ok- got a cold but trying to...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>- halla!!! doing ok- got a cold but trying to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>49ec0f5742</td>\n",
       "      <td>... thx for ur msg, so awesome!  luv the new ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>so awesome!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>97e53162ff</td>\n",
       "      <td>dear oh dear.....</td>\n",
       "      <td>negative</td>\n",
       "      <td>dear oh dear.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>ca8ae20678</td>\n",
       "      <td>valium makes you feel goood. i need more. i ca...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>valium makes you feel goood. i need more. i c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>c281b14817</td>\n",
       "      <td>last class at 10:30. One final tomorrow and 2 ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>last class at 10:30. one final tomorrow and 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>c9dfbf4275</td>\n",
       "      <td>I have a new found respect for you now that I...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i have a new found respect for you now that i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>91c44d5912</td>\n",
       "      <td>What`s all your fault?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>what`s all your fault?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>81defce8a3</td>\n",
       "      <td>http://twitter.com/friends?page=20 press pre...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>http://twitter.com/friends?page=20 press prev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>04c455e820</td>\n",
       "      <td>Does `Real Detroit Weekly` not have a website....</td>\n",
       "      <td>negative</td>\n",
       "      <td>oh the horror, the horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>48a90031bd</td>\n",
       "      <td>got a sniffle, got the kids and hubby just lef...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>got a sniffle, got the kids and hubby just le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>d9e1b10a4f</td>\n",
       "      <td>wow.. tomorrow and then it`s over. i`ll never ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>it`s kind of sad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>932af12853</td>\n",
       "      <td>Hey! Let`s Follow each other! Wouldn`t that ju...</td>\n",
       "      <td>positive</td>\n",
       "      <td>awesome!?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>389a3e653c</td>\n",
       "      <td>no problem! i think it`s a great thing to ref...</td>\n",
       "      <td>positive</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>6dc1aa9db8</td>\n",
       "      <td>hey Gerardo! (late response) that day i was t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>upset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2963</th>\n",
       "      <td>08ad89c6b0</td>\n",
       "      <td>Hi .. I have the net YAYY.. Im here for a shor...</td>\n",
       "      <td>positive</td>\n",
       "      <td>yayy..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>90be8eaffb</td>\n",
       "      <td>I want a new phone  I`ve seen too much cellpho...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i`ve seen too much cellphone commercials</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                               text sentiment  \\\n",
       "2094  d60072b03a  He can`t fix it.   I guess I`ll write until I ...  negative   \n",
       "1176  3cb4d10927           Watching WALL-E.....it`s so cute but sad   neutral   \n",
       "2774  5fb30f858b   Probably not, kinda expensive and we have to ...  negative   \n",
       "480   819626535b   Glad to hear you made it out, I hear that pla...  positive   \n",
       "3189  1a857a38fb  Weekend is getting close. Too bad I`ll be stuc...  positive   \n",
       "2246  676b733c57  Buying pretty shiny beads and things  I feel q...   neutral   \n",
       "2418  49c713c76d         Is It The Bit Where Hollie Started Crying?   neutral   \n",
       "1184  d2141d6d47                                        Thats great  positive   \n",
       "907   b3fa6e5c24   I always hope they will die out but then i se...  negative   \n",
       "2181  8f851c59f6   - halla!!! doing ok- got a cold but trying to...   neutral   \n",
       "1562  49ec0f5742   ... thx for ur msg, so awesome!  luv the new ...  positive   \n",
       "401   97e53162ff                                  dear oh dear.....  negative   \n",
       "2718  ca8ae20678  valium makes you feel goood. i need more. i ca...   neutral   \n",
       "1393  c281b14817  last class at 10:30. One final tomorrow and 2 ...   neutral   \n",
       "1848  c9dfbf4275   I have a new found respect for you now that I...   neutral   \n",
       "2119  91c44d5912                             What`s all your fault?   neutral   \n",
       "1911  81defce8a3    http://twitter.com/friends?page=20 press pre...   neutral   \n",
       "2921  04c455e820  Does `Real Detroit Weekly` not have a website....  negative   \n",
       "1740  48a90031bd  got a sniffle, got the kids and hubby just lef...   neutral   \n",
       "635   d9e1b10a4f  wow.. tomorrow and then it`s over. i`ll never ...  negative   \n",
       "1808  932af12853  Hey! Let`s Follow each other! Wouldn`t that ju...  positive   \n",
       "2076  389a3e653c   no problem! i think it`s a great thing to ref...  positive   \n",
       "2538  6dc1aa9db8   hey Gerardo! (late response) that day i was t...  negative   \n",
       "2963  08ad89c6b0  Hi .. I have the net YAYY.. Im here for a shor...  positive   \n",
       "307   90be8eaffb  I want a new phone  I`ve seen too much cellpho...  negative   \n",
       "\n",
       "                                          selected_text  \n",
       "2094                                              lame.  \n",
       "1176           watching wall-e.....it`s so cute but sad  \n",
       "2774                                          expensive  \n",
       "480                                                glad  \n",
       "3189   hopefully i`ll be able to get out next weeken...  \n",
       "2246   buying pretty shiny beads and things i feel q...  \n",
       "2418         is it the bit where hollie started crying?  \n",
       "1184                                        thats great  \n",
       "907                                                 sad  \n",
       "2181   - halla!!! doing ok- got a cold but trying to...  \n",
       "1562                                        so awesome!  \n",
       "401                                   dear oh dear.....  \n",
       "2718   valium makes you feel goood. i need more. i c...  \n",
       "1393   last class at 10:30. one final tomorrow and 2...  \n",
       "1848   i have a new found respect for you now that i...  \n",
       "2119                             what`s all your fault?  \n",
       "1911   http://twitter.com/friends?page=20 press prev...  \n",
       "2921                          oh the horror, the horror  \n",
       "1740   got a sniffle, got the kids and hubby just le...  \n",
       "635                                   it`s kind of sad.  \n",
       "1808                                          awesome!?  \n",
       "2076                                              great  \n",
       "2538                                              upset  \n",
       "2963                                             yayy..  \n",
       "307            i`ve seen too much cellphone commercials  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show 25 random predicted 'selected_text'\n",
    "test.sample(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
